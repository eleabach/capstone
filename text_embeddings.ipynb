{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Class for text vectorization\n",
        "In this notebook, we create a class for creating embeddings using different techniques. \n",
        "\n",
        "To test our functions, we'll use the MeDAL dataset. This dataset is available in [Hugging Face](https://huggingface.co/datasets/medal) which was published in the [paper](https://aclanthology.org/2020.clinicalnlp-1.15/)."
      ],
      "metadata": {
        "id": "VNPq2_avyACm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install libraries\n",
        "!pip install datasets\n",
        "!pip install wget\n",
        "!pip install transformers\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "kS-r63tY0mus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load libraries\n",
        "import wget\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "import gensim.downloader as api\n",
        "from datasets import load_dataset\n",
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "from gensim.utils import simple_preprocess\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "metadata": {
        "id": "syanwde0z0Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data containing text data\n",
        "dataset = load_dataset(\"wikitext\", 'wikitext-103-v1')"
      ],
      "metadata": {
        "id": "vYsAxSbv-kTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the word2vec model\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "gDBqg44vQ15v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the transformers model\n",
        "model = SentenceTransformer('bert-base-uncased')"
      ],
      "metadata": {
        "id": "uMzO77WS6bMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Class"
      ],
      "metadata": {
        "id": "pchiRDvbkw2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.terminal import embed\n",
        "class Create_embedding:\n",
        "    \"\"\"\n",
        "    A class used to convert text into embeddings\n",
        "    using various techniques.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text : str\n",
        "        text for which we want the embedding.\n",
        "    path_to_model : str\n",
        "        pass a model object or \n",
        "        path to the dir where a custom model is saved or name of model.\n",
        "        All possible hugging face models can be found here : https://huggingface.co/models?library=sentence-transformers&sort=downloads\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    word_2_vec\n",
        "        Return word2vec embeddings for the text.\n",
        "    transformers\n",
        "        Returner emedding by any transformer in huggingface\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, text, path_to_model=None):\n",
        "      self.text = text\n",
        "      self.path_to_model = path_to_model\n",
        "      self.clean_text = self.preprocess()\n",
        "\n",
        "    def __str__(self):\n",
        "      return \"This class will create word embedding for:\\n\"+str(self.text[0:100])\n",
        "\n",
        "    def preprocess(self):\n",
        "      self.clean_text = simple_preprocess(self.text)\n",
        "      return self.clean_text\n",
        "    \n",
        "    def word_2_vec(self, model=None):\n",
        "      # check if a model object is passed:\n",
        "      if self.path_to_model.word_vec!=None:\n",
        "        # get the vector\n",
        "        self.vector = np.mean([self.path_to_model[word] for word in self.clean_text if word in self.path_to_model.vocab], axis=0)\n",
        "      else:\n",
        "        raise Exception(\"Model not found. Please load a word2vec model.\")\n",
        "        self.vector = np.mean([model[word] for word in self.clean_text if word in model.vocab], axis=0)\n",
        "      return self.vector\n",
        "      \n",
        "\n",
        "    def transformers(self, model = None):\n",
        "      '''\n",
        "      inputs : mode_name, name of the model we want to use. Examples are : 'bert-base-uncased', 'paraphrase-MiniLM-L6-v2'..\n",
        "      returns embeddings by a transformer of the text, default is bert, but other model can be specified by its name in hugginface\n",
        "      or by the path_to_model class attribute\n",
        "      '''\n",
        "      if model == None:\n",
        "        model  = 'bert-base-uncased'\n",
        "      if self.path_to_model==None:\n",
        "        model = SentenceTransformer(model)\n",
        "      else:\n",
        "        model = self.path_to_model\n",
        "      embedding = model.encode(self.clean_text)\n",
        "      embedding = np.mean(embedding, axis=0)\n",
        "      return(embedding)\n",
        "\n"
      ],
      "metadata": {
        "id": "RnGe1goMVdP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing embedding class\n",
        "\n",
        "## Word2Vec"
      ],
      "metadata": {
        "id": "qXB4HbGrk1dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataframe with 10 sentences \n",
        "test_df = dataset['train']['text'][:500]\n",
        "sentences = []\n",
        "for index, i in enumerate(test_df):\n",
        "  if len(i.split())>200:\n",
        "    sentences.append(i)\n",
        "# covert to pandas dataframe\n",
        "test_df = pd.DataFrame({\"Sno\":list(range(35)),\"Text\":sentences})"
      ],
      "metadata": {
        "id": "u2grZSNI652u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = test_df['Text'][5]\n",
        "print (example_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyMW2WCT52SU",
        "outputId": "f59a7aa7-0a90-49aa-a18f-c5df0ed173c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The majority of material created for previous games , such as the <unk> system and the design of maps , was carried over . Alongside this , improvements were made to the game 's graphics and some elements were expanded , such as map layouts , mission structure , and the number of playable units per mission . A part of this upgrade involved creating unique polygon models for each character 's body . In order to achieve this , the cooperative elements incorporated into the second game were removed , as they took up a large portion of memory space needed for the improvements . They also adjusted the difficulty settings and ease of play so they could appeal to new players while retaining the essential components of the series ' gameplay . The newer systems were decided upon early in development . The character designs were done by <unk> Honjou , who had worked on the previous Valkyria Chronicles games . When creating the Nameless Squad , Honjou was faced with the same problem he had had during the first game : the military uniforms essentially destroyed character individuality , despite him needing to create unique characters the player could identify while maintaining a sense of reality within the Valkyria Chronicles world . The main color of the Nameless was black . As with the previous Valkyria games , Valkyria Chronicles III used the <unk> graphics engine . The anime opening was produced by Production I.G. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the instance of the class\n",
        "c = Create_embedding(example_text, wv)"
      ],
      "metadata": {
        "id": "4jIt6iTsfo8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call word2vec method\n",
        "sent_emb = c.word_2_vec()"
      ],
      "metadata": {
        "id": "UyfKr7Fhhbkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_emb[0:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKeuhhR9km7s",
        "outputId": "f99b9794-c936-44d3-a1e3-8c8c30b351af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.04658379,  0.04597364,  0.03893401,  0.05313334, -0.0451308 ,\n",
              "       -0.03555235,  0.02645404, -0.07672491,  0.04935056,  0.04654928,\n",
              "       -0.01989292, -0.08623394, -0.00842675,  0.03092166, -0.06884254,\n",
              "       -0.00228381,  0.01326396,  0.06409685, -0.01029554, -0.04073678,\n",
              "       -0.0405631 ,  0.03694576, -0.05718748, -0.00582404,  0.03121588,\n",
              "       -0.02216141, -0.07861641,  0.08075499,  0.00866464, -0.01616759,\n",
              "       -0.03417229, -0.02988266, -0.00700652,  0.02845157,  0.03449544,\n",
              "       -0.00767024,  0.02535533, -0.02963617,  0.04576784,  0.03250948,\n",
              "        0.0963238 ,  0.00183121,  0.06422134,  0.03469758,  0.00072397,\n",
              "       -0.06504219, -0.03479271,  0.01023298,  0.03527363,  0.02942501],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer model"
      ],
      "metadata": {
        "id": "ou6fiPOE9kCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('bert-base-uncased')"
      ],
      "metadata": {
        "id": "PFeplLGS1Cfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## with pre loaded model\n",
        "c = Create_embedding(example_text,model)\n",
        "c.transformers()[0:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh4QvBQF1_a4",
        "outputId": "82cbbbb4-f122-49af-9f8c-5c91b86bf27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.11990342, -0.07570083, -0.1506623 ,  0.01293704, -0.02102345,\n",
              "       -0.1435671 ,  0.30146962, -0.09403347,  0.05861866, -0.26148862,\n",
              "       -0.11637579, -0.00111108,  0.01206072,  0.12087645, -0.23695384,\n",
              "       -0.16093141, -0.01133005,  0.10137323,  0.05568448,  0.12851872,\n",
              "        0.15672764,  0.01832988,  0.10598295,  0.07400321,  0.10141755,\n",
              "        0.22951035, -0.20914586, -0.10933466, -0.26989615,  0.07977872,\n",
              "       -0.04152232, -0.1268066 , -0.04269395,  0.32250774, -0.1554826 ,\n",
              "       -0.22229369,  0.04874475, -0.04411151, -0.3464809 , -0.0945673 ,\n",
              "       -0.08143158, -0.0672645 ,  0.08026835, -0.07770447,  0.02715283,\n",
              "       -0.09066958, -0.24938029,  0.04231898, -0.24726741,  0.22496547],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## with model name\n",
        "c = Create_embedding(example_text)\n",
        "c.transformers('paraphrase-MiniLM-L6-v2')[0:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI57L8HW1I4F",
        "outputId": "21f37f27-d0e2-4ec6-cee7-4b707e7e7985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.21894754,  0.4953562 ,  0.44741312, ..., -0.1363442 ,\n",
              "        -0.09861054,  0.19872642],\n",
              "       [ 0.09742697,  0.45453203,  0.5956033 , ..., -0.2156922 ,\n",
              "        -0.30407894,  0.38588253],\n",
              "       [ 0.7213939 ,  0.27405986,  0.59295493, ...,  0.6663704 ,\n",
              "         0.08657774,  0.19664077],\n",
              "       [ 0.43469286,  0.02567542,  0.18255733, ..., -0.41889527,\n",
              "         0.98770744, -0.12975271],\n",
              "       [ 0.22393198,  0.16315515,  0.08766361, ..., -0.06896546,\n",
              "         0.48545623,  0.0731098 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ]
}